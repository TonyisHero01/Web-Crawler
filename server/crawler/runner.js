const path = require('path');
const { spawn } = require('child_process');
const { pool } = require('../db/conn');

const jobs = {};

const jobManager = {
  /**
   * å¯åŠ¨å®šæ—¶çˆ¬è™«ä»»åŠ¡ï¼ˆå®šæ—¶æ‰§è¡Œ runPythonCrawlerï¼‰
   */
  schedule: (url, depth, pattern, website_record_id, interval_seconds) => {
    const idStr = website_record_id.toString();
    jobManager.stop(idStr); // å…ˆåœæ­¢æ—§çš„ä»»åŠ¡ï¼ˆç¡®ä¿å”¯ä¸€ï¼‰

    const job = setInterval(() => {
      runPythonCrawler(url, depth, pattern, website_record_id);
    }, interval_seconds * 1000);

    jobs[idStr] = job;
    console.log(`ðŸ“… Scheduled job for #${idStr}`);
  },

  /**
   * åœæ­¢æŒ‡å®šä»»åŠ¡
   */
  stop: (website_record_id) => {
    const idStr = website_record_id.toString();
    if (jobs[idStr]) {
      clearInterval(jobs[idStr]);
      delete jobs[idStr];
      console.log(`ðŸ›‘ Stopped job for #${idStr}`);
    }
  },

  /**
   * æ¸…é™¤æ‰€æœ‰ä»»åŠ¡
   */
  clear: () => {
    for (let id in jobs) {
      clearInterval(jobs[id]);
      console.log(`ðŸ§¹ Cleared job for #${id}`);
    }
    Object.keys(jobs).forEach(id => delete jobs[id]);
  },

  /**
   * æŸ¥çœ‹å½“å‰æ‰€æœ‰æ´»è·ƒä»»åŠ¡ï¼ˆå¯é€‰æ‰©å±•ï¼‰
   */
  list: () => {
    return Object.keys(jobs);
  }
};


/**
 * çœŸæ­£è°ƒç”¨ Python è„šæœ¬æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
 */
async function runPythonCrawler(url, depth, pattern, websiteRecordId) {
  console.log(`[CRAWLER] â–¶ï¸ Starting crawl for ${url} (depth: ${depth}, pattern: ${pattern})`);

  const result = await pool.query(
    `INSERT INTO executions (website_record_id) VALUES ($1) RETURNING id`,
    [websiteRecordId]
  );
  const executionId = result.rows[0].id;

  const pythonPath = 'python'; // æˆ– 'python3'
  const scriptPath = path.join(__dirname, '../crawler.py');
  const args = [scriptPath, url, depth.toString(), pattern || '.*', executionId.toString()];
  const py = spawn(pythonPath, args);

  let insertedCount = 0;

  py.stdout.on("data", (data) => {
    console.log("[PYTHON OUTPUT]", data.toString());
    if (data.toString().startsWith('[INSERTED]')) {
      insertedCount += 1;
    }
  });

  py.stderr.on("data", (data) => {
    console.error("[PYTHON ERROR]", data.toString());
  });

  py.on("close", async () => {
    await pool.query(
      `UPDATE executions SET end_time = CURRENT_TIMESTAMP, crawled_count = $1 WHERE id = $2`,
      [insertedCount, executionId]
    );
    console.log(`[EXECUTION COMPLETE] ID ${executionId} | ${insertedCount} pages`);
  });
}

module.exports = { jobManager, runPythonCrawler };